"""
AI Consultant - –ú–æ–¥—É–ª—å AI-–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç–∞ –¥–ª—è promarkirui.ru
–í–µ—Ä—Å–∏—è 2.0: Document-First Architecture —Å Router
"""

import os
import json
import uuid
import sqlite3
import httpx
import subprocess
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Annotated, TypedDict, Literal
from contextlib import contextmanager

from fastapi import APIRouter, HTTPException, Request, Depends, BackgroundTasks
from fastapi.responses import FileResponse
from pydantic import BaseModel

# LangGraph imports
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.tools import tool
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from langgraph.graph.message import add_messages

# ==================== –ù–ê–°–¢–†–û–ô–ö–ò ====================

ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')
TELEGRAM_BOT_TOKEN = os.getenv('TELEGRAM_BOT_TOKEN', '')
TELEGRAM_CHAT_ID = os.getenv('TELEGRAM_CHAT_ID', '')

# –ü—É—Ç–∏ –∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
REGULATIONS_DIR = '/var/www/promarkirui/backend/knowledge_base/regulations/'
VED_DIR = '/var/www/promarkirui/backend/knowledge_base/ved/'

# –¶–µ–Ω—ã –Ω–∞ —Ç–æ–∫–µ–Ω—ã Claude (–∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤)
PRICING = {
    'claude-sonnet-4-20250514': {'input': 3.0, 'output': 15.0},
    'claude-3-5-sonnet-20241022': {'input': 3.0, 'output': 15.0},
    'claude-3-haiku-20240307': {'input': 0.25, 'output': 1.25},
}

# ==================== –ú–ê–ü–ü–ò–ù–ì –î–û–ö–£–ú–ï–ù–¢–û–í ====================

# –ú–∞–ø–ø–∏–Ω–≥ —Ç–æ–≤–∞—Ä–æ–≤ –Ω–∞ —Ñ–∞–π–ª—ã –ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–π
PRODUCT_TO_REGULATION = {
    # –°–ø–æ—Ä—Ç–∏–≤–Ω–æ–µ –ø–∏—Ç–∞–Ω–∏–µ
    '—Å–ø–æ—Ä—Ç–ø–∏—Ç': {'file_pattern': '811', 'pp_number': '811', 'name': '—Å–ø–æ—Ä—Ç–∏–≤–Ω–æ–µ –ø–∏—Ç–∞–Ω–∏–µ'},
    '—Å–ø–æ—Ä—Ç–∏–≤–Ω–æ–µ –ø–∏—Ç–∞–Ω–∏–µ': {'file_pattern': '811', 'pp_number': '811', 'name': '—Å–ø–æ—Ä—Ç–∏–≤–Ω–æ–µ –ø–∏—Ç–∞–Ω–∏–µ'},
    '–ø—Ä–æ—Ç–µ–∏–Ω': {'file_pattern': '811', 'pp_number': '811', 'name': '—Å–ø–æ—Ä—Ç–∏–≤–Ω–æ–µ –ø–∏—Ç–∞–Ω–∏–µ'},

    # –û–±—É–≤—å
    '–æ–±—É–≤—å': {'file_pattern': '860', 'pp_number': '860', 'name': '–æ–±—É–≤—å'},
    '–∫—Ä–æ—Å—Å–æ–≤–∫–∏': {'file_pattern': '860', 'pp_number': '860', 'name': '–æ–±—É–≤—å'},
    '–±–æ—Ç–∏–Ω–∫–∏': {'file_pattern': '860', 'pp_number': '860', 'name': '–æ–±—É–≤—å'},
    '—Ç—É—Ñ–ª–∏': {'file_pattern': '860', 'pp_number': '860', 'name': '–æ–±—É–≤—å'},

    # –ú–æ–ª–æ—á–∫–∞
    '–º–æ–ª–æ–∫–æ': {'file_pattern': '2099', 'pp_number': '2099', 'name': '–º–æ–ª–æ—á–Ω–∞—è –ø—Ä–æ–¥—É–∫—Ü–∏—è'},
    '–º–æ–ª–æ—á–∫–∞': {'file_pattern': '2099', 'pp_number': '2099', 'name': '–º–æ–ª–æ—á–Ω–∞—è –ø—Ä–æ–¥—É–∫—Ü–∏—è'},
    '—Å—ã—Ä': {'file_pattern': '2099', 'pp_number': '2099', 'name': '–º–æ–ª–æ—á–Ω–∞—è –ø—Ä–æ–¥—É–∫—Ü–∏—è'},
    '—Ç–≤–æ—Ä–æ–≥': {'file_pattern': '2099', 'pp_number': '2099', 'name': '–º–æ–ª–æ—á–Ω–∞—è –ø—Ä–æ–¥—É–∫—Ü–∏—è'},
    '–π–æ–≥—É—Ä—Ç': {'file_pattern': '2099', 'pp_number': '2099', 'name': '–º–æ–ª–æ—á–Ω–∞—è –ø—Ä–æ–¥—É–∫—Ü–∏—è'},
    '—Å–º–µ—Ç–∞–Ω–∞': {'file_pattern': '2099', 'pp_number': '2099', 'name': '–º–æ–ª–æ—á–Ω–∞—è –ø—Ä–æ–¥—É–∫—Ü–∏—è'},
    '–º–æ—Ä–æ–∂–µ–Ω–æ–µ': {'file_pattern': '2099', 'pp_number': '2099', 'name': '–º–æ–ª–æ—á–Ω–∞—è –ø—Ä–æ–¥—É–∫—Ü–∏—è'},

    # –û–¥–µ–∂–¥–∞ / –ª–µ–≥–ø—Ä–æ–º
    '–æ–¥–µ–∂–¥–∞': {'file_pattern': '1956', 'pp_number': '1956', 'name': '—Ç–æ–≤–∞—Ä—ã –ª—ë–≥–∫–æ–π –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ—Å—Ç–∏'},
    '—Ç–µ–∫—Å—Ç–∏–ª—å': {'file_pattern': '1956', 'pp_number': '1956', 'name': '—Ç–æ–≤–∞—Ä—ã –ª—ë–≥–∫–æ–π –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ—Å—Ç–∏'},
    '–∫—É—Ä—Ç–∫–∞': {'file_pattern': '1956', 'pp_number': '1956', 'name': '—Ç–æ–≤–∞—Ä—ã –ª—ë–≥–∫–æ–π –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ—Å—Ç–∏'},
    '–ø–∞–ª—å—Ç–æ': {'file_pattern': '1956', 'pp_number': '1956', 'name': '—Ç–æ–≤–∞—Ä—ã –ª—ë–≥–∫–æ–π –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ—Å—Ç–∏'},
    '–±–µ–ª—å–µ': {'file_pattern': '1956', 'pp_number': '1956', 'name': '—Ç–æ–≤–∞—Ä—ã –ª—ë–≥–∫–æ–π –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ—Å—Ç–∏'},
    '–ø–æ—Å—Ç–µ–ª—å–Ω–æ–µ': {'file_pattern': '1956', 'pp_number': '1956', 'name': '—Ç–æ–≤–∞—Ä—ã –ª—ë–≥–∫–æ–π –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ—Å—Ç–∏'},

    # –ü–∏–≤–æ
    '–ø–∏–≤–æ': {'file_pattern': '2173', 'pp_number': '2173', 'name': '–ø–∏–≤–æ –∏ —Å–ª–∞–±–æ–∞–ª–∫–æ–≥–æ–ª—å–Ω—ã–µ –Ω–∞–ø–∏—Ç–∫–∏'},
    '–ø–∏–≤–Ω—ã–µ –Ω–∞–ø–∏—Ç–∫–∏': {'file_pattern': '2173', 'pp_number': '2173', 'name': '–ø–∏–≤–æ'},

    # –î–µ—Ç—Å–∫–∏–µ —Ç–æ–≤–∞—Ä—ã
    '–¥–µ—Ç—Å–∫–∏–µ —Ç–æ–≤–∞—Ä—ã': {'file_pattern': '819', 'pp_number': '819', 'name': '—Ç–æ–≤–∞—Ä—ã –¥–ª—è –¥–µ—Ç–µ–π'},
    '–∏–≥—Ä—É—à–∫–∏': {'file_pattern': '819', 'pp_number': '819', 'name': '—Ç–æ–≤–∞—Ä—ã –¥–ª—è –¥–µ—Ç–µ–π'},
    '–¥–µ—Ç—Å–∫–∞—è –æ–¥–µ–∂–¥–∞': {'file_pattern': '819', 'pp_number': '819', 'name': '—Ç–æ–≤–∞—Ä—ã –¥–ª—è –¥–µ—Ç–µ–π'},

    # –°—Ç—Ä–æ–π–º–∞—Ç–µ—Ä–∏–∞–ª—ã
    '—Å—Ç—Ä–æ–π–º–∞—Ç–µ—Ä–∏–∞–ª—ã': {'file_pattern': '820', 'pp_number': '820', 'name': '—Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã'},
    '—Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã': {'file_pattern': '820', 'pp_number': '820', 'name': '—Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã'},

    # –®–∏–Ω—ã
    '—à–∏–Ω—ã': {'file_pattern': '1958', 'pp_number': '1958', 'name': '—à–∏–Ω—ã –∏ –ø–æ–∫—Ä—ã—à–∫–∏'},
    '–ø–æ–∫—Ä—ã—à–∫–∏': {'file_pattern': '1958', 'pp_number': '1958', 'name': '—à–∏–Ω—ã –∏ –ø–æ–∫—Ä—ã—à–∫–∏'},
    '–∞–≤—Ç–æ—à–∏–Ω—ã': {'file_pattern': '1958', 'pp_number': '1958', 'name': '—à–∏–Ω—ã –∏ –ø–æ–∫—Ä—ã—à–∫–∏'},

    # –õ–µ–∫–∞—Ä—Å—Ç–≤–∞
    '–ª–µ–∫–∞—Ä—Å—Ç–≤–∞': {'file_pattern': '1556', 'pp_number': '1556', 'name': '–ª–µ–∫–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–µ–ø–∞—Ä–∞—Ç—ã'},
    '–ª–µ–∫–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–µ–ø–∞—Ä–∞—Ç—ã': {'file_pattern': '1556', 'pp_number': '1556', 'name': '–ª–µ–∫–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–µ–ø–∞—Ä–∞—Ç—ã'},
    '–º–µ–¥–∏–∫–∞–º–µ–Ω—Ç—ã': {'file_pattern': '1556', 'pp_number': '1556', 'name': '–ª–µ–∫–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–µ–ø–∞—Ä–∞—Ç—ã'},

    # –¢–∞–±–∞–∫
    '—Ç–∞–±–∞–∫': {'file_pattern': '224', 'pp_number': '224', 'name': '—Ç–∞–±–∞—á–Ω–∞—è –ø—Ä–æ–¥—É–∫—Ü–∏—è'},
    '—Å–∏–≥–∞—Ä–µ—Ç—ã': {'file_pattern': '224', 'pp_number': '224', 'name': '—Ç–∞–±–∞—á–Ω–∞—è –ø—Ä–æ–¥—É–∫—Ü–∏—è'},
    '–≤–µ–π–ø': {'file_pattern': '224', 'pp_number': '224', 'name': '—Ç–∞–±–∞—á–Ω–∞—è –ø—Ä–æ–¥—É–∫—Ü–∏—è'},

    # –ú–µ—Ö–∞
    '–º–µ—Ö': {'file_pattern': '787', 'pp_number': '787', 'name': '–º–µ—Ö–æ–≤—ã–µ –∏–∑–¥–µ–ª–∏—è'},
    '—à—É–±–∞': {'file_pattern': '787', 'pp_number': '787', 'name': '–º–µ—Ö–æ–≤—ã–µ –∏–∑–¥–µ–ª–∏—è'},
    '–º–µ—Ö–æ–≤—ã–µ –∏–∑–¥–µ–ª–∏—è': {'file_pattern': '787', 'pp_number': '787', 'name': '–º–µ—Ö–æ–≤—ã–µ –∏–∑–¥–µ–ª–∏—è'},

    # –ü–∞—Ä—Ñ—é–º–µ—Ä–∏—è
    '–ø–∞—Ä—Ñ—é–º–µ—Ä–∏—è': {'file_pattern': '1957', 'pp_number': '1957', 'name': '–ø–∞—Ä—Ñ—é–º–µ—Ä–∏—è'},
    '–¥—É—Ö–∏': {'file_pattern': '1957', 'pp_number': '1957', 'name': '–ø–∞—Ä—Ñ—é–º–µ—Ä–∏—è'},
    '—Ç—É–∞–ª–µ—Ç–Ω–∞—è –≤–æ–¥–∞': {'file_pattern': '1957', 'pp_number': '1957', 'name': '–ø–∞—Ä—Ñ—é–º–µ—Ä–∏—è'},

    # –§–æ—Ç–æ—Ç–æ–≤–∞—Ä—ã
    '—Ñ–æ—Ç–æ–∞–ø–ø–∞—Ä–∞—Ç': {'file_pattern': '1953', 'pp_number': '1953', 'name': '—Ñ–æ—Ç–æ—Ç–æ–≤–∞—Ä—ã'},
    '—Ñ–æ—Ç–æ—Ç–æ–≤–∞—Ä—ã': {'file_pattern': '1953', 'pp_number': '1953', 'name': '—Ñ–æ—Ç–æ—Ç–æ–≤–∞—Ä—ã'},

    # –í–æ–¥–∞
    '–≤–æ–¥–∞': {'file_pattern': '841', 'pp_number': '841', 'name': '—É–ø–∞–∫–æ–≤–∞–Ω–Ω–∞—è –≤–æ–¥–∞'},
    '–º–∏–Ω–µ—Ä–∞–ª—å–Ω–∞—è –≤–æ–¥–∞': {'file_pattern': '841', 'pp_number': '841', 'name': '—É–ø–∞–∫–æ–≤–∞–Ω–Ω–∞—è –≤–æ–¥–∞'},
    '–ø–∏—Ç—å–µ–≤–∞—è –≤–æ–¥–∞': {'file_pattern': '841', 'pp_number': '841', 'name': '—É–ø–∞–∫–æ–≤–∞–Ω–Ω–∞—è –≤–æ–¥–∞'},

    # –°–æ–∫–∏
    '—Å–æ–∫–∏': {'file_pattern': '887', 'pp_number': '887', 'name': '—Å–æ–∫–∏ –∏ –Ω–∞–ø–∏—Ç–∫–∏'},
    '–Ω–∞–ø–∏—Ç–∫–∏': {'file_pattern': '887', 'pp_number': '887', 'name': '—Å–æ–∫–∏ –∏ –Ω–∞–ø–∏—Ç–∫–∏'},

    # –ë–ê–î
    '–±–∞–¥': {'file_pattern': '886', 'pp_number': '886', 'name': '–ë–ê–î'},
    '–≤–∏—Ç–∞–º–∏–Ω—ã': {'file_pattern': '886', 'pp_number': '886', 'name': '–ë–ê–î'},
    '–¥–æ–±–∞–≤–∫–∏': {'file_pattern': '886', 'pp_number': '886', 'name': '–ë–ê–î'},

    # –ò–∫—Ä–∞
    '–∏–∫—Ä–∞': {'file_pattern': '2028', 'pp_number': '2028', 'name': '–∏–∫—Ä–∞'},
    '–∫—Ä–∞—Å–Ω–∞—è –∏–∫—Ä–∞': {'file_pattern': '2028', 'pp_number': '2028', 'name': '–∏–∫—Ä–∞'},
    '—á–µ—Ä–Ω–∞—è –∏–∫—Ä–∞': {'file_pattern': '2028', 'pp_number': '2028', 'name': '–∏–∫—Ä–∞'},

    # –í–µ–ª–æ—Å–∏–ø–µ–¥—ã
    '–≤–µ–ª–æ—Å–∏–ø–µ–¥': {'file_pattern': '645', 'pp_number': '645', 'name': '–≤–µ–ª–æ—Å–∏–ø–µ–¥—ã'},
    '–≤–µ–ª–æ—Å–∏–ø–µ–¥—ã': {'file_pattern': '645', 'pp_number': '645', 'name': '–≤–µ–ª–æ—Å–∏–ø–µ–¥—ã'},

    # –ö–æ—Ä–º–∞
    '–∫–æ—Ä–º': {'file_pattern': '674', 'pp_number': '674', 'name': '–∫–æ—Ä–º–∞ –¥–ª—è –∂–∏–≤–æ—Ç–Ω—ã—Ö'},
    '–∫–æ—Ä–º–∞ –¥–ª—è –∂–∏–≤–æ—Ç–Ω—ã—Ö': {'file_pattern': '674', 'pp_number': '674', 'name': '–∫–æ—Ä–º–∞ –¥–ª—è –∂–∏–≤–æ—Ç–Ω—ã—Ö'},
    '–∫–æ—Ä–º –¥–ª—è —Å–æ–±–∞–∫': {'file_pattern': '674', 'pp_number': '674', 'name': '–∫–æ—Ä–º–∞ –¥–ª—è –∂–∏–≤–æ—Ç–Ω—ã—Ö'},
    '–∫–æ—Ä–º –¥–ª—è –∫–æ—à–µ–∫': {'file_pattern': '674', 'pp_number': '674', 'name': '–∫–æ—Ä–º–∞ –¥–ª—è –∂–∏–≤–æ—Ç–Ω—ã—Ö'},

    # –ú–∞—Å–ª–æ
    '–º–∞—Å–ª–æ': {'file_pattern': '676', 'pp_number': '676', 'name': '—Ä–∞—Å—Ç–∏—Ç–µ–ª—å–Ω—ã–µ –º–∞—Å–ª–∞'},
    '—Ä–∞—Å—Ç–∏—Ç–µ–ª—å–Ω–æ–µ –º–∞—Å–ª–æ': {'file_pattern': '676', 'pp_number': '676', 'name': '—Ä–∞—Å—Ç–∏—Ç–µ–ª—å–Ω—ã–µ –º–∞—Å–ª–∞'},
    '–ø–æ–¥—Å–æ–ª–Ω–µ—á–Ω–æ–µ –º–∞—Å–ª–æ': {'file_pattern': '676', 'pp_number': '676', 'name': '—Ä–∞—Å—Ç–∏—Ç–µ–ª—å–Ω—ã–µ –º–∞—Å–ª–∞'},

    # –ö–æ–Ω—Å–µ—Ä–≤—ã
    '–∫–æ–Ω—Å–µ—Ä–≤—ã': {'file_pattern': '677', 'pp_number': '677', 'name': '–∫–æ–Ω—Å–µ—Ä–≤—ã'},

    # –ë–µ–∑–∞–ª–∫–æ–≥–æ–ª—å–Ω–æ–µ –ø–∏–≤–æ
    '–±–µ–∑–∞–ª–∫–æ–≥–æ–ª—å–Ω–æ–µ –ø–∏–≤–æ': {'file_pattern': '678', 'pp_number': '678', 'name': '–±–µ–∑–∞–ª–∫–æ–≥–æ–ª—å–Ω–æ–µ –ø–∏–≤–æ'},

    # –ê–Ω—Ç–∏—Å–µ–ø—Ç–∏–∫–∏
    '–∞–Ω—Ç–∏—Å–µ–ø—Ç–∏–∫': {'file_pattern': '870', 'pp_number': '870', 'name': '–∞–Ω—Ç–∏—Å–µ–ø—Ç–∏–∫–∏'},
    '–¥–µ–∑–∏–Ω—Ñ–∏—Ü–∏—Ä—É—é—â–∏–µ —Å—Ä–µ–¥—Å—Ç–≤–∞': {'file_pattern': '870', 'pp_number': '870', 'name': '–∞–Ω—Ç–∏—Å–µ–ø—Ç–∏–∫–∏'},

    # –ú–µ–¥–∏–∑–¥–µ–ª–∏—è
    '–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –∏–∑–¥–µ–ª–∏—è': {'file_pattern': '894', 'pp_number': '894', 'name': '–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –∏–∑–¥–µ–ª–∏—è'},
    '–º–µ–¥–∏–∑–¥–µ–ª–∏—è': {'file_pattern': '894', 'pp_number': '894', 'name': '–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –∏–∑–¥–µ–ª–∏—è'},

    # –ö—Ä–µ—Å–ª–∞-–∫–æ–ª—è—Å–∫–∏
    '–∫–æ–ª—è—Å–∫–∞': {'file_pattern': '885', 'pp_number': '885', 'name': '–∫—Ä–µ—Å–ª–∞-–∫–æ–ª—è—Å–∫–∏'},
    '–∫—Ä–µ—Å–ª–æ-–∫–æ–ª—è—Å–∫–∞': {'file_pattern': '885', 'pp_number': '885', 'name': '–∫—Ä–µ—Å–ª–∞-–∫–æ–ª—è—Å–∫–∏'},

    # –°–ª–∞–¥–æ—Å—Ç–∏
    '—Å–ª–∞–¥–æ—Å—Ç–∏': {'file_pattern': '818', 'pp_number': '818', 'name': '–∫–æ–Ω–¥–∏—Ç–µ—Ä—Å–∫–∏–µ –∏–∑–¥–µ–ª–∏—è'},
    '–∫–æ–Ω—Ñ–µ—Ç—ã': {'file_pattern': '818', 'pp_number': '818', 'name': '–∫–æ–Ω–¥–∏—Ç–µ—Ä—Å–∫–∏–µ –∏–∑–¥–µ–ª–∏—è'},
    '—à–æ–∫–æ–ª–∞–¥': {'file_pattern': '818', 'pp_number': '818', 'name': '–∫–æ–Ω–¥–∏—Ç–µ—Ä—Å–∫–∏–µ –∏–∑–¥–µ–ª–∏—è'},

    # –ë–∞–∫–∞–ª–µ—è
    '–±–∞–∫–∞–ª–µ—è': {'file_pattern': '1682', 'pp_number': '1682', 'name': '–±–∞–∫–∞–ª–µ—è'},
    '–∫—Ä—É–ø—ã': {'file_pattern': '1682', 'pp_number': '1682', 'name': '–±–∞–∫–∞–ª–µ—è'},
    '–º–∞–∫–∞—Ä–æ–Ω—ã': {'file_pattern': '1682', 'pp_number': '1682', 'name': '–±–∞–∫–∞–ª–µ—è'},
    '—Å–Ω–µ–∫–∏': {'file_pattern': '1682', 'pp_number': '1682', 'name': '–±–∞–∫–∞–ª–µ—è'},

    # –°–º–∞–∑–æ—á–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã
    '–º–æ—Ç–æ—Ä–Ω–æ–µ –º–∞—Å–ª–æ': {'file_pattern': '1683', 'pp_number': '1683', 'name': '—Å–º–∞–∑–æ—á–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã'},
    '–∞–Ω—Ç–∏—Ñ—Ä–∏–∑': {'file_pattern': '1683', 'pp_number': '1683', 'name': '—Å–º–∞–∑–æ—á–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã'},
    '–∞–≤—Ç–æ–∂–∏–¥–∫–æ—Å—Ç–∏': {'file_pattern': '1683', 'pp_number': '1683', 'name': '—Å–º–∞–∑–æ—á–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã'},

    # –ë—ã—Ç–æ–≤–∞—è —Ö–∏–º–∏—è
    '–±—ã—Ç–æ–≤–∞—è —Ö–∏–º–∏—è': {'file_pattern': '1681', 'pp_number': '1681', 'name': '–±—ã—Ç–æ–≤–∞—è —Ö–∏–º–∏—è'},
    '–º–æ—é—â–∏–µ —Å—Ä–µ–¥—Å—Ç–≤–∞': {'file_pattern': '1681', 'pp_number': '1681', 'name': '–±—ã—Ç–æ–≤–∞—è —Ö–∏–º–∏—è'},

    # –í–µ—Ç–µ—Ä–∏–Ω–∞—Ä–Ω—ã–µ –ª–µ–∫–∞—Ä—Å—Ç–≤–∞
    '–≤–µ—Ç–µ—Ä–∏–Ω–∞—Ä–Ω—ã–µ –ª–µ–∫–∞—Ä—Å—Ç–≤–∞': {'file_pattern': '675', 'pp_number': '675', 'name': '–≤–µ—Ç–µ—Ä–∏–Ω–∞—Ä–Ω—ã–µ –ø—Ä–µ–ø–∞—Ä–∞—Ç—ã'},
    '–ª–µ–∫–∞—Ä—Å—Ç–≤–∞ –¥–ª—è –∂–∏–≤–æ—Ç–Ω—ã—Ö': {'file_pattern': '675', 'pp_number': '675', 'name': '–≤–µ—Ç–µ—Ä–∏–Ω–∞—Ä–Ω—ã–µ –ø—Ä–µ–ø–∞—Ä–∞—Ç—ã'},
}

# –ú–∞–ø–ø–∏–Ω–≥ —Ç–µ–º –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã –í–≠–î
TOPIC_TO_VED_DOC = {
    '—à—Ç—Ä–∞—Ñ': {'file': 'koap_rf.txt', 'name': '–ö–æ–ê–ü –†–§'},
    '–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å': {'file': 'koap_rf.txt', 'name': '–ö–æ–ê–ü –†–§'},
    '–Ω–∞–∫–∞–∑–∞–Ω–∏–µ': {'file': 'koap_rf.txt', 'name': '–ö–æ–ê–ü –†–§'},

    '–Ω–∞–ª–æ–≥': {'file': 'nk_rf.txt', 'name': '–ù–∞–ª–æ–≥–æ–≤—ã–π –∫–æ–¥–µ–∫—Å –†–§'},
    '–Ω–¥—Å': {'file': 'nk_rf.txt', 'name': '–ù–∞–ª–æ–≥–æ–≤—ã–π –∫–æ–¥–µ–∫—Å –†–§'},

    '–±—É—Ö–≥–∞–ª—Ç–µ—Ä': {'file': 'pbu_3_2006.txt', 'name': '–ü–ë–£ 3/2006'},
    '–ø—Ä–æ–≤–æ–¥–∫': {'file': 'pbu_3_2006.txt', 'name': '–ü–ë–£ 3/2006'},
    '—É—á—ë—Ç': {'file': 'pbu_3_2006.txt', 'name': '–ü–ë–£ 3/2006'},
    '—É—á–µ—Ç': {'file': 'pbu_3_2006.txt', 'name': '–ü–ë–£ 3/2006'},

    '—Ç–∞–º–æ–∂–Ω': {'file': 'tk_eaes.txt', 'name': '–¢–∞–º–æ–∂–µ–Ω–Ω—ã–π –∫–æ–¥–µ–∫—Å –ï–ê–≠–°'},
    '—Ä–∞—Å—Ç–∞–º–æ–∂': {'file': 'fz_289_customs.txt', 'name': '–§–ó-289 –æ —Ç–∞–º–æ–∂–µ–Ω–Ω–æ–º —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–∏'},
    '–∏–º–ø–æ—Ä—Ç': {'file': 'fz_289_customs.txt', 'name': '–§–ó-289 –æ —Ç–∞–º–æ–∂–µ–Ω–Ω–æ–º —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–∏'},
}

# –¢–µ–º—ã –í–ù–ï –Ω–∞—à–µ–π –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ ‚Äî —Å—Ä–∞–∑—É –æ—Ç–∫–ª–æ–Ω—è–µ–º (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ù–ï —Å–≤—è–∑–∞–Ω–æ —Å –º–∞—Ä–∫–∏—Ä–æ–≤–∫–æ–π)
OUT_OF_SCOPE_TOPICS = [
    '–±–∞–Ω–∫', '–∫—Ä–µ–¥–∏—Ç', '–ª–∏–∑–∏–Ω–≥',
    '—é—Ä–∏—Å—Ç', '—Å—É–¥', '–∏—Å–∫',
    '1—Å', '–±–∏—Ç—Ä–∏–∫—Å', '—Å–∞–π—Ç', '–ø—Ä–æ–≥—Ä–∞–º–º'
]

# –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
DETAIL_KEYWORDS = [
    '–æ—Å—Ç–∞—Ç–∫–∏', '–æ—Å—Ç–∞—Ç–æ–∫', '–ø–µ—Ä–µ—Ö–æ–¥–Ω—ã–π –ø–µ—Ä–∏–æ–¥', '–∏—Å–∫–ª—é—á–µ–Ω–∏—è', '–∏—Å–∫–ª—é—á–µ–Ω–∏–µ',
    '–¥–ª—è –∫–æ–≥–æ', '–∫—Ç–æ –æ–±—è–∑–∞–Ω', '–∫—Ç–æ –¥–æ–ª–∂–µ–Ω', '–Ω—é–∞–Ω—Å—ã', '–æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏',
    '—Å—Ä–æ–∫–∏', '–¥–∞—Ç–∞', '–∫–æ–≥–¥–∞', '—Å –∫–∞–∫–æ–≥–æ', '–¥–æ –∫–∞–∫–æ–≥–æ', '—ç—Ç–∞–ø—ã',
    '—à—Ç—Ä–∞—Ñ', '–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å', '–Ω–∞–∫–∞–∑–∞–Ω–∏–µ', '—Å–∞–Ω–∫—Ü–∏–∏',
    '—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è', '–ø—Ä–∞–≤–∏–ª–∞', '–ø–æ—Ä—è–¥–æ–∫', '–∫–∞–∫', '—á—Ç–æ –¥–µ–ª–∞—Ç—å'
]

# ==================== –ë–ê–ó–ê –î–ê–ù–ù–´–• ====================

DB_PATH = '/var/www/promarkirui/backend/promarkirui.db'

@contextmanager
def get_db():
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    try:
        yield conn
    finally:
        conn.close()

# ==================== –ú–û–î–ï–õ–ò API ====================

class ChatMessage(BaseModel):
    message: str
    session_id: Optional[str] = None
    current_page: Optional[str] = None
    page_title: Optional[str] = None

class SettingsUpdate(BaseModel):
    setting_key: str
    setting_value: str

class KnowledgeItem(BaseModel):
    category: str
    question: Optional[str] = None
    answer: str
    keywords: Optional[str] = None
    priority: int = 0

class QuickReply(BaseModel):
    trigger_words: str
    response: str

class DataSourceCreate(BaseModel):
    name: str
    source_type: str
    category: str
    description: str
    file_path: Optional[str] = None
    is_active: bool = True

# ==================== –§–†–ê–ó–´-–ü–ê–£–ó–´ ====================

import random

PAUSE_PHRASES_SEARCH = [
    "–°–µ–∫—É–Ω–¥–æ—á–∫—É, –≥–ª—è–Ω—É –≤ –ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–∏...",
    "–°–µ–π—á–∞—Å —É—Ç–æ—á–Ω—é –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º...",
    "–ú–∏–Ω—É—Ç–∫—É, –ø—Ä–æ–≤–µ—Ä—è—é...",
    "–°–µ–∫—É–Ω–¥—É, —Å–º–æ—Ç—Ä—é –≤ –Ω–æ—Ä–º–∞—Ç–∏–≤–∫–µ...",
    "–°–µ–π—á–∞—Å –≥–ª—è–Ω—É...",
]

PAUSE_PHRASES_VED = [
    "–°–µ–∫—É–Ω–¥—É, –ø–æ—Å–º–æ—Ç—Ä—é –≤ –∫–æ–¥–µ–∫—Å–µ...",
    "–°–µ–π—á–∞—Å –ø—Ä–æ–≤–µ—Ä—é –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö...",
    "–ú–∏–Ω—É—Ç–∫—É, —É—Ç–æ—á–Ω—è—é –ø–æ –∑–∞–∫–æ–Ω–∞–º...",
]

PAUSE_PHRASES_THINKING = [
    "–•–º, —Å–µ–∫—É–Ω–¥—É...",
    "–¢–∞–∫, —Å–µ–π—á–∞—Å...",
    "–ú–∏–Ω—É—Ç–∫—É...",
]

def get_pause_phrase(msg_type: str = "general") -> str:
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ª—É—á–∞–π–Ω—É—é —Ñ—Ä–∞–∑—É-–ø–∞—É–∑—É"""
    if msg_type == "document_search":
        return random.choice(PAUSE_PHRASES_SEARCH)
    elif msg_type == "ved_search":
        return random.choice(PAUSE_PHRASES_VED)
    else:
        return random.choice(PAUSE_PHRASES_THINKING)

def calculate_typing_delay(text: str, has_search: bool = False) -> int:
    """
    –†–∞—Å—Å—á–∏—Ç–∞—Ç—å –∑–∞–¥–µ—Ä–∂–∫—É "–ø–µ—á–∞—Ç–∏" –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö.
    –ß–µ–º –¥–ª–∏–Ω–Ω–µ–µ —Ç–µ–∫—Å—Ç ‚Äî —Ç–µ–º –±–æ–ª—å—à–µ –ø–∞—É–∑–∞.
    –ï—Å–ª–∏ –±—ã–ª –ø–æ–∏—Å–∫ ‚Äî –¥–æ–±–∞–≤–ª—è–µ–º –≤—Ä–µ–º—è –Ω–∞ "–ø—Ä–æ—Å–º–æ—Ç—Ä".

    –í–ê–ñ–ù–û: –î–∞—ë–º —á–µ–ª–æ–≤–µ–∫—É –≤—Ä–µ–º—è –Ω–∞–ø–∏—Å–∞—Ç—å –µ—â—ë —Å–æ–æ–±—â–µ–Ω–∏–µ,
    –ø–æ—ç—Ç–æ–º—É –ø–∞—É–∑–∞ –Ω–µ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–π.
    """
    base_delay = 1500  # –±–∞–∑–æ–≤–∞—è –ø–∞—É–∑–∞ ‚Äî 1.5 —Å–µ–∫ –º–∏–Ω–∏–º—É–º
    char_delay = 15    # ~15–º—Å –Ω–∞ —Å–∏–º–≤–æ–ª (—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –ø–µ—á–∞—Ç–∏)
    search_delay = 2000  # –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø–∞—É–∑–∞ –µ—Å–ª–∏ –±—ã–ª –ø–æ–∏—Å–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö

    delay = base_delay + len(text) * char_delay

    if has_search:
        delay += search_delay

    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º: –º–∏–Ω–∏–º—É–º 1.5—Å–µ–∫, –º–∞–∫—Å–∏–º—É–º 6—Å–µ–∫
    return max(1500, min(delay, 6000))


# ==================== DOCUMENT-FIRST ROUTER ====================

# –ü—Ä–∏–∑–Ω–∞–∫–∏ –Ω–µ–ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞ (—Ç–æ–ª—å–∫–æ –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç—Å—è, –Ω–µ –ø–æ–∫—É–ø–∞–µ—Ç)
LOW_PRIORITY_SIGNALS = [
    '–ø—Ä–æ—Å—Ç–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ', '–ø—Ä–æ—Å—Ç–æ —Å–ø—Ä–æ—Å–∏—Ç—å', '–ª—é–±–æ–ø—ã—Ç–Ω–æ', '–¥–ª—è —Å–µ–±—è',
    '—Å–∞–º–æ–∑–∞–Ω—è—Ç—ã–π', '—Ñ–∏–∑–ª–∏—Ü–æ', '—Ñ–∏–∑–∏–∫', '—á–∞—Å—Ç–Ω–æ–µ –ª–∏—Ü–æ',
    '–æ–¥–∏–Ω —Ç–æ–≤–∞—Ä', '1 —Ç–æ–≤–∞—Ä', '–ø–∞—Ä—É —à—Ç—É–∫', '–Ω–µ—Å–∫–æ–ª—å–∫–æ —à—Ç—É–∫',
    '—Å—Ç—É–¥–µ–Ω—Ç', '–∫—É—Ä—Å–æ–≤–∞—è', '–¥–∏–ø–ª–æ–º', '—Ä–µ—Ñ–µ—Ä–∞—Ç'
]

# –ü—Ä–∏–∑–Ω–∞–∫–∏ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞ (–±–∏–∑–Ω–µ—Å, –æ–±—ä—ë–º—ã)
HIGH_PRIORITY_SIGNALS = [
    '–∏–ø', '–æ–æ–æ', '–∫–æ–º–ø–∞–Ω–∏—è', '–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è', '–ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏–µ', '—Ñ–∏—Ä–º–∞',
    '–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ', '–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º', '–∏–º–ø–æ—Ä—Ç', '–∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º', '–æ–ø—Ç–æ–º', '–æ–ø—Ç',
    '–ø–∞—Ä—Ç–∏—è', '–æ–±—ä—ë–º', '–æ–±—ä–µ–º—ã', '—Ç—ã—Å—è—á', '–º–Ω–æ–≥–æ', '–∫—Ä—É–ø–Ω—ã–π',
    '—Å–∫–ª–∞–¥', '–º–∞–≥–∞–∑–∏–Ω', '—Å–µ—Ç—å', '—Ç–æ—Ä–≥–æ–≤–ª—è', '–ø—Ä–æ–¥–∞—ë–º', '–ø—Ä–æ–¥–∞–µ–º',
    '—Å—Ä–æ—á–Ω–æ', '—à—Ç—Ä–∞—Ñ', '–ø—Ä–æ–≤–µ—Ä–∫–∞', '–∫–æ–Ω—Ç—Ä–æ–ª—å'
]

def classify_message(message: str) -> Dict:
    """
    –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–æ–æ–±—â–µ–Ω–∏–π ‚Äî –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω—É–∂–µ–Ω –ª–∏ –ø–æ–∏—Å–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö.
    –†–∞–±–æ—Ç–∞–µ—Ç –ë–ï–ó LLM –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤.
    """
    message_lower = message.lower()

    result = {
        'type': 'general',  # greeting, product_question, ved_question, general
        'needs_document_search': False,
        'product': None,
        'regulation_info': None,
        'ved_doc': None,
        'search_query': None,
        'client_priority': 'medium',  # low, medium, high
        'suggest_self_service': False  # –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å —Å–∞–π—Ç –¥–ª—è —Å–∞–º–æ–ø—Ä–æ–≤–µ—Ä–∫–∏
    }

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∫–ª–∏–µ–Ω—Ç–∞
    low_signals = sum(1 for s in LOW_PRIORITY_SIGNALS if s in message_lower)
    high_signals = sum(1 for s in HIGH_PRIORITY_SIGNALS if s in message_lower)

    if low_signals > high_signals:
        result['client_priority'] = 'low'
        result['suggest_self_service'] = True
    elif high_signals > 0:
        result['client_priority'] = 'high'

    # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è
    greetings = ['–ø—Ä–∏–≤–µ—Ç', '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π', '–¥–æ–±—Ä—ã–π –¥–µ–Ω—å', '–¥–æ–±—Ä—ã–π –≤–µ—á–µ—Ä', '–¥–æ–±—Ä–æ–µ —É—Ç—Ä–æ', '—Ö–∞–π', 'hello', 'hi']
    if any(g in message_lower for g in greetings) and len(message_lower) < 30:
        result['type'] = 'greeting'
        return result

    # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–æ–ø—Ä–æ—Å—ã –æ —Ç–æ–≤–∞—Ä–∞—Ö (—Å —É—á—ë—Ç–æ–º —á–∞—Å—Ç–∏—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π)
    for product_key, reg_info in PRODUCT_TO_REGULATION.items():
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞—á–∞–ª–æ —Å–ª–æ–≤–∞ (–æ–±—É–≤ -> –æ–±—É–≤—å, –æ–±—É–≤–∏, –æ–±—É–≤—å—é –∏ —Ç.–¥.)
        key_root = product_key[:4] if len(product_key) >= 4 else product_key
        if key_root in message_lower or product_key in message_lower:
            result['type'] = 'product_question'
            result['product'] = product_key
            result['regulation_info'] = reg_info

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω—É–∂–µ–Ω –ª–∏ –¥–µ—Ç–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫
            if any(kw in message_lower for kw in DETAIL_KEYWORDS):
                result['needs_document_search'] = True
                result['search_query'] = message

            return result

    # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–æ–ø—Ä–æ—Å—ã –ø–æ –í–≠–î/–Ω–∞–ª–æ–≥–∞–º/—à—Ç—Ä–∞—Ñ–∞–º
    for topic_key, doc_info in TOPIC_TO_VED_DOC.items():
        if topic_key in message_lower:
            result['type'] = 'ved_question'
            result['ved_doc'] = doc_info
            result['needs_document_search'] = True
            result['search_query'] = message
            return result

    # 4. –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –º–∞—Ä–∫–∏—Ä–æ–≤–∫–∏
    marking_keywords = ['–º–∞—Ä–∫–∏—Ä–æ–≤–∫–∞', '–º–∞—Ä–∫–∏—Ä—É–µ—Ç—Å—è', '—á–µ—Å—Ç–Ω—ã–π –∑–Ω–∞–∫', '—á–∑', '–æ–±—è–∑–∞—Ç–µ–ª—å–Ω']
    if any(kw in message_lower for kw in marking_keywords):
        result['type'] = 'marking_question'
        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Ç–æ–≤–∞—Ä –≤ –≤–æ–ø—Ä–æ—Å–µ
        for product_key, reg_info in PRODUCT_TO_REGULATION.items():
            if product_key in message_lower:
                result['product'] = product_key
                result['regulation_info'] = reg_info
                result['needs_document_search'] = True
                result['search_query'] = message
                break
        return result

    # 5. –ü—Ä–æ—Å—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã "–Ω—É–∂–Ω–∞ –ª–∏ –º–∞—Ä–∫–∏—Ä–æ–≤–∫–∞" –±–µ–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä–∞
    simple_check_keywords = ['–Ω—É–∂–Ω–∞ –ª–∏', '–ø–æ–¥–ª–µ–∂–∏—Ç –ª–∏', '–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞ –ª–∏', '–º–∞—Ä–∫–∏—Ä–æ–≤–∞—Ç—å –ª–∏', '–ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ç–æ–≤–∞—Ä']
    if any(kw in message_lower for kw in simple_check_keywords):
        result['type'] = 'simple_check'
        result['suggest_self_service'] = True
        return result

    return result


def find_regulation_file(pattern: str) -> Optional[str]:
    """–ù–∞–π—Ç–∏ —Ñ–∞–π–ª –ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É (–Ω–æ–º–µ—Ä—É –ü–ü)"""
    try:
        for filename in os.listdir(REGULATIONS_DIR):
            if pattern in filename and filename.endswith('.rtf'):
                return os.path.join(REGULATIONS_DIR, filename)
    except Exception as e:
        print(f'[ROUTER] Error finding file: {e}')
    return None


def search_in_rtf(file_path: str, query: str, max_fragments: int = 3) -> List[str]:
    """–ü–æ–∏—Å–∫ –≤ RTF —Ñ–∞–π–ª–µ —á–µ—Ä–µ–∑ catdoc (–ª—É—á—à–µ –¥–ª—è —Ä—É—Å—Å–∫–∏—Ö RTF —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π cp1251)"""
    results = []
    query_words = [w for w in query.lower().split() if len(w) > 3]

    # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
    search_patterns = query_words + ['–¥–æ–ø—É—Å–∫–∞–µ—Ç—Å—è', '–≤–ø—Ä–∞–≤–µ', '–æ–±—è–∑–∞–Ω', '—É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è',
                                      '–ø–æ 31', '—Å 1 ', '–¥–æ ', '–Ω–∞—á–∏–Ω–∞—è —Å', '–Ω–µ –ø–æ–∑–¥–Ω–µ–µ',
                                      '–º–∞—Ä–∫–∏—Ä–æ–≤–∫', '—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü', '—Å—Ä–æ–∫', '–¥–∞—Ç–∞']

    try:
        # catdoc –ª—É—á—à–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å —Ä—É—Å—Å–∫–∏–º–∏ RTF —Ñ–∞–π–ª–∞–º–∏ –≤ –∫–æ–¥–∏—Ä–æ–≤–∫–µ cp1251
        result = subprocess.run(
            ['catdoc', '-d', 'utf-8', file_path],
            capture_output=True, text=True, timeout=15
        )
        content = result.stdout

        # Fallback –Ω–∞ unrtf –µ—Å–ª–∏ catdoc –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª
        if not content or result.returncode != 0:
            result = subprocess.run(
                ['unrtf', '--text', file_path],
                capture_output=True, timeout=15
            )
            # –ü—Ä–æ–±—É–µ–º –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∫ cp1251
            try:
                content = result.stdout.decode('cp1251')
            except:
                content = result.stdout.decode('utf-8', errors='ignore')

        if not content:
            print(f'[SEARCH_RTF] No content from file: {file_path}')
            return []

        lines = content.split('\n')
        print(f'[SEARCH_RTF] Read {len(lines)} lines from {os.path.basename(file_path)}')

        for i, line in enumerate(lines):
            line_lower = line.lower()

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏
            if any(pattern in line_lower for pattern in search_patterns):
                start = max(0, i - 2)
                end = min(len(lines), i + 5)
                fragment = ' '.join(lines[start:end]).strip()

                # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ
                if len(fragment) > 100 and fragment[:100] not in str(results):
                    results.append(fragment[:700])
                    if len(results) >= max_fragments:
                        break

        print(f'[SEARCH_RTF] Found {len(results)} fragments')

    except Exception as e:
        print(f'[SEARCH_RTF] Error: {e}')
        import traceback
        traceback.print_exc()

    return results


def search_in_ved_doc(doc_file: str, query: str) -> List[str]:
    """–ü–æ–∏—Å–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –í–≠–î (txt —Ñ–∞–π–ª—ã)"""
    file_path = os.path.join(VED_DIR, doc_file)
    results = []

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        lines = content.split('\n')
        query_words = [w for w in query.lower().split() if len(w) > 3]

        for i, line in enumerate(lines):
            line_lower = line.lower()
            matches = sum(1 for word in query_words if word in line_lower)

            if matches >= min(2, len(query_words)):
                start = max(0, i - 3)
                end = min(len(lines), i + 5)
                fragment = '\n'.join(lines[start:end])

                if len(fragment) > 100:
                    results.append(fragment[:1000])
                    if len(results) >= 3:
                        break

    except Exception as e:
        print(f'[SEARCH_VED] Error: {e}')

    return results


# ==================== LANGGRAPH AGENT STATE ====================

class AgentState(TypedDict):
    messages: Annotated[list, add_messages]
    classification: Optional[Dict]  # –†–µ–∑—É–ª—å—Ç–∞—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    document_context: Optional[str]  # –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    pending_message: Optional[str]  # –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –∫–ª–∏–µ–Ω—Ç—É

# ==================== –ò–ù–°–¢–†–£–ú–ï–ù–¢–´ –ê–ì–ï–ù–¢–ê ====================

# –õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ —Ç–æ–≤–∞—Ä–∞—Ö
_categories_data = None
_products_lookup = None

def get_categories_data():
    global _categories_data
    if _categories_data is None:
        try:
            from server import CATEGORIES_DATA
            _categories_data = CATEGORIES_DATA
        except ImportError:
            _categories_data = []
    return _categories_data

def get_products_lookup():
    global _products_lookup
    if _products_lookup is None:
        try:
            from server import PRODUCTS_LOOKUP
            _products_lookup = PRODUCTS_LOOKUP
        except ImportError:
            _products_lookup = {}
    return _products_lookup

@tool
def search_product(query: str) -> str:
    """
    –ü–æ–∏—Å–∫ —Ç–æ–≤–∞—Ä–∞ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é. –ò—Å–ø–æ–ª—å–∑—É–π –∫–æ–≥–¥–∞ –∫–ª–∏–µ–Ω—Ç —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç –æ —Ç–æ–≤–∞—Ä–µ.

    Args:
        query: –ù–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞ (–æ–±—É–≤—å, –º–æ–ª–æ–∫–æ, –æ–¥–µ–∂–¥–∞ –∏ —Ç.–¥.)
    """
    query_lower = query.lower()
    results = []
    categories = get_categories_data()

    for category in categories:
        for subcategory in category.get("subcategories", []):
            if query_lower in subcategory.get("name", "").lower() or query_lower in category.get("name", "").lower():
                for product in subcategory.get("products", [])[:3]:
                    status = product.get("marking_status", "unknown")
                    since = product.get("mandatory_since", "")

                    status_text = {
                        "mandatory": "–û–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è",
                        "voluntary": "–î–æ–±—Ä–æ–≤–æ–ª—å–Ω–∞—è",
                        "planned": "–ü–ª–∞–Ω–∏—Ä—É–µ—Ç—Å—è"
                    }.get(status, "?")

                    results.append(f"‚Ä¢ {product.get('name')} (–¢–ù –í–≠–î: {product.get('tnved')}) ‚Äî {status_text}" + (f", —Å {since}" if since else ""))

                if len(results) >= 5:
                    break

    if results:
        return f"–ù–∞–π–¥–µ–Ω–æ –ø–æ –∑–∞–ø—Ä–æ—Å—É '{query}':\n" + "\n".join(results[:5])
    return f"–¢–æ–≤–∞—Ä '{query}' –Ω–µ –Ω–∞–π–¥–µ–Ω. –£—Ç–æ—á–Ω–∏—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏–µ."


@tool
def check_tnved(code: str) -> str:
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –∫–æ–¥—É –¢–ù –í–≠–î.

    Args:
        code: –ö–æ–¥ –¢–ù –í–≠–î (–Ω–∞–ø—Ä–∏–º–µ—Ä 6403, 0401)
    """
    code = code.strip().replace(" ", "")
    products = get_products_lookup()

    for product_id, product in products.items():
        tnved = product.get("tnved", "").replace(" ", "")
        if tnved.startswith(code) or code.startswith(tnved[:4]):
            status = product.get("marking_status")
            since = product.get("mandatory_since", "")
            timeline = product.get("timeline") or {}

            if status == "mandatory":
                reqs = timeline.get("current_requirements") or ["–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –≤ –ß–µ—Å—Ç–Ω–æ–º –ó–ù–ê–ö–µ"]
                start_date = since or timeline.get('start_date', '—É—Ç–æ—á–Ω—è–π—Ç–µ')
                return f"""–ö–æ–¥ {code} ‚Äî –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–ê–Ø –º–∞—Ä–∫–∏—Ä–æ–≤–∫–∞!

{product.get('name')}
–°: {start_date}

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
""" + "\n".join(f"‚Ä¢ {r}" for r in reqs[:4])

            elif status == "voluntary":
                return f"–ö–æ–¥ {code} ‚Äî –¥–æ–±—Ä–æ–≤–æ–ª—å–Ω–∞—è –º–∞—Ä–∫–∏—Ä–æ–≤–∫–∞. –¢–æ–≤–∞—Ä: {product.get('name')}"

            elif status == "planned":
                return f"–ö–æ–¥ {code} ‚Äî –º–∞—Ä–∫–∏—Ä–æ–≤–∫–∞ –ø–ª–∞–Ω–∏—Ä—É–µ—Ç—Å—è —Å {since}. –¢–æ–≤–∞—Ä: {product.get('name')}"

    return f"–ö–æ–¥ {code} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –±–∞–∑–µ –º–∞—Ä–∫–∏—Ä—É–µ–º—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤."


@tool
def get_fines() -> str:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —à—Ç—Ä–∞—Ñ–∞—Ö –∑–∞ –Ω–∞—Ä—É—à–µ–Ω–∏—è –º–∞—Ä–∫–∏—Ä–æ–≤–∫–∏."""
    return """–®–¢–†–ê–§–´ –∑–∞ –Ω–∞—Ä—É—à–µ–Ω–∏—è –º–∞—Ä–∫–∏—Ä–æ–≤–∫–∏:

–ü—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ/–≤–≤–æ–∑ –±–µ–∑ –º–∞—Ä–∫–∏—Ä–æ–≤–∫–∏:
‚Ä¢ –Æ—Ä–ª–∏—Ü–∞: 50 000 - 100 000 —Ä—É–± + –∫–æ–Ω—Ñ–∏—Å–∫–∞—Ü–∏—è
‚Ä¢ –ò–ü: 5 000 - 10 000 —Ä—É–±

–ü—Ä–æ–¥–∞–∂–∞ –±–µ–∑ –º–∞—Ä–∫–∏—Ä–æ–≤–∫–∏:
‚Ä¢ –Æ—Ä–ª–∏—Ü–∞: 50 000 - 300 000 —Ä—É–± + –∫–æ–Ω—Ñ–∏—Å–∫–∞—Ü–∏—è
‚Ä¢ –ò–ü: 5 000 - 10 000 —Ä—É–±

–£–≥–æ–ª–æ–≤–Ω–∞—è –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å (–æ—Ç 1,5 –º–ª–Ω —Ä—É–±): –¥–æ 3 –ª–µ—Ç

–®—Ç—Ä–∞—Ñ—ã –∑–∞ –ö–ê–ñ–î–£–Æ –µ–¥–∏–Ω–∏—Ü—É —Ç–æ–≤–∞—Ä–∞!"""




_created_requests = set()

@tool
def create_request(
    name: str,
    phone: str,
    product: str = "",
    volume: str = "",
    city: str = "",
    has_experience: str = "",
    request_type: str = "–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è"
) -> str:
    """
    –°–æ–∑–¥–∞—Ç—å –∑–∞—è–≤–∫—É. –í—ã–∑—ã–≤–∞–π –¢–û–õ–¨–ö–û –∫–æ–≥–¥–∞ –∫–ª–∏–µ–Ω—Ç –¥–∞–ª –∏–º—è –ò —Ç–µ–ª–µ—Ñ–æ–Ω.

    Args:
        name: –ò–º—è –∫–ª–∏–µ–Ω—Ç–∞ (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ)
        phone: –¢–µ–ª–µ—Ñ–æ–Ω –∫–ª–∏–µ–Ω—Ç–∞ (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ)
        product: –ö–∞–∫–æ–π —Ç–æ–≤–∞—Ä (–µ—Å–ª–∏ –∏–∑–≤–µ—Å—Ç–Ω–æ)
        volume: –û–±—ä—ë–º (–µ—Å–ª–∏ –∏–∑–≤–µ—Å—Ç–Ω–æ)
        city: –ì–æ—Ä–æ–¥ (–µ—Å–ª–∏ –∏–∑–≤–µ—Å—Ç–Ω–æ)
        has_experience: –û–ø—ã—Ç —Å –º–∞—Ä–∫–∏—Ä–æ–≤–∫–æ–π (–µ—Å–ª–∏ –∏–∑–≤–µ—Å—Ç–Ω–æ)
        request_type: –¢–∏–ø –∑–∞—è–≤–∫–∏
    """
    phone_clean = ''.join(c for c in phone if c.isdigit())[-10:]
    if phone_clean in _created_requests:
        return f"–ó–∞—è–≤–∫–∞ –¥–ª—è {name} —É–∂–µ –ø–µ—Ä–µ–¥–∞–Ω–∞ —ç–∫—Å–ø–µ—Ä—Ç–∞–º!"

    # –§–æ—Ä–º–∏—Ä—É–µ–º —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–µ —Ä–µ–∑—é–º–µ –¥–∏–∞–ª–æ–≥–∞
    summary_parts = []

    # –û—Ç–∫—É–¥–∞ –∫–ª–∏–µ–Ω—Ç
    if city and city not in ['', '–Ω–µ —É–∫–∞–∑–∞–Ω–æ']:
        summary_parts.append(f"–ö–ª–∏–µ–Ω—Ç –∏–∑ {city}")
    else:
        summary_parts.append("–ö–ª–∏–µ–Ω—Ç")

    # –ü—Ä–æ–¥—É–∫—Ü–∏—è
    if product and product not in ['', '–Ω–µ —É–∫–∞–∑–∞–Ω–æ']:
        summary_parts.append(f"–ø—Ä–æ–¥—É–∫—Ü–∏—è ‚Äî {product}")

    # –û–±—ä—ë–º
    if volume and volume not in ['', '–Ω–µ —É–∫–∞–∑–∞–Ω–æ']:
        summary_parts.append(f"–æ–±—ä—ë–º {volume}")

    # –û–ø—ã—Ç
    if has_experience and has_experience not in ['', '–Ω–µ —É–∫–∞–∑–∞–Ω–æ']:
        if has_experience.lower() in ['–Ω–µ—Ç', '–ø–µ—Ä–≤—ã–π —Ä–∞–∑', '–≤–ø–µ—Ä–≤—ã–µ', '–Ω–µ —Ä–∞–±–æ—Ç–∞–ª–∏']:
            summary_parts.append("—Ç–æ–ª—å–∫–æ –Ω–∞—á–∏–Ω–∞–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Å –º–∞—Ä–∫–∏—Ä–æ–≤–∫–æ–π")
        elif has_experience.lower() in ['–¥–∞', '–µ—Å—Ç—å', '—Ä–∞–±–æ—Ç–∞–µ–º', '—Ä–∞–±–æ—Ç–∞–ª–∏']:
            summary_parts.append("—É–∂–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –º–∞—Ä–∫–∏—Ä–æ–≤–∫–æ–π")
        else:
            summary_parts.append(f"–æ–ø—ã—Ç: {has_experience}")

    # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—é–º–µ
    summary = ", ".join(summary_parts) + "."

    # –î–ª—è CRM ‚Äî —á–∏—Å—Ç–æ–µ —Ä–µ–∑—é–º–µ
    crm_comment = f"{summary}\n\n–ó–∞–ø—Ä–æ—Å: {request_type}"

    # –î–ª—è Telegram ‚Äî –∫—Ä–∞—Å–∏–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
    tg_message = f"""ü§ñ <b>–ó–∞—è–≤–∫–∞ –æ—Ç AI-–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç–∞</b>

<b>{name}</b> ‚Äî {phone}

{summary}

<i>–ó–∞–ø—Ä–æ—Å: {request_type}</i>"""

    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ API —Å –∏—Å—Ç–æ—á–Ω–∏–∫–æ–º ai_consultant
    try:
        response = httpx.post(
            "http://localhost:8001/api/contact/send",
            json={
                "name": name,
                "phone": phone,
                "request_type": request_type,
                "comment": crm_comment,
                "source": "ai_consultant",  # –ò—Å—Ç–æ—á–Ω–∏–∫ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                "consent": True
            },
            timeout=10
        )

        if response.status_code == 200:
            _created_requests.add(phone_clean)

            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ Telegram
            if TELEGRAM_BOT_TOKEN and TELEGRAM_CHAT_ID:
                try:
                    httpx.post(
                        f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage",
                        json={"chat_id": TELEGRAM_CHAT_ID, "text": tg_message, "parse_mode": "HTML"},
                        timeout=5
                    )
                except:
                    pass

            return f"–û—Ç–ª–∏—á–Ω–æ, {name}! –ó–∞—è–≤–∫–∞ –ø–µ—Ä–µ–¥–∞–Ω–∞. –≠–∫—Å–ø–µ—Ä—Ç –ø–µ—Ä–µ–∑–≤–æ–Ω–∏—Ç –≤ —Ç–µ—á–µ–Ω–∏–µ 15 –º–∏–Ω—É—Ç."
        return f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â—ë —Ä–∞–∑ –∏–ª–∏ –ø–æ–∑–≤–æ–Ω–∏—Ç–µ –Ω–∞–º."
    except Exception as e:
        _created_requests.add(phone_clean)
        return f"–ó–∞—è–≤–∫–∞ –ø—Ä–∏–Ω—è—Ç–∞, {name}! –°–∫–æ—Ä–æ –ø–µ—Ä–µ–∑–≤–æ–Ω–∏–º."


@tool
def call_manager(client_info: str, question: str, summary: str) -> str:
    """
    –ü–æ–∑–≤–∞—Ç—å –º–µ–Ω–µ–¥–∂–µ—Ä–∞.

    Args:
        client_info: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–ª–∏–µ–Ω—Ç–µ
        question: –í–æ–ø—Ä–æ—Å
        summary: –†–µ–∑—é–º–µ –¥–∏–∞–ª–æ–≥–∞
    """
    if TELEGRAM_BOT_TOKEN and TELEGRAM_CHAT_ID:
        try:
            msg = f"–ó–∞–ø—Ä–æ—Å –æ—Ç AI-–±–æ—Ç–∞\n\n{client_info}\n–í–æ–ø—Ä–æ—Å: {question}\n{summary}"
            httpx.post(
                f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage",
                json={"chat_id": TELEGRAM_CHAT_ID, "text": msg},
                timeout=10
            )
        except:
            pass

    return "–ú–µ–Ω–µ–¥–∂–µ—Ä —É–≤–µ–¥–æ–º–ª—ë–Ω –∏ —Å–∫–æ—Ä–æ —Å–≤—è–∂–µ—Ç—Å—è!"


# ==================== –°–ü–†–ê–í–û–ß–ù–ò–ö –ü–û–°–¢–ê–ù–û–í–õ–ï–ù–ò–ô ====================

_regulations_cache = None

def load_regulations():
    global _regulations_cache
    if _regulations_cache is not None:
        return _regulations_cache

    try:
        with open('/var/www/promarkirui/backend/knowledge_base/regulations.json', 'r', encoding='utf-8') as f:
            _regulations_cache = json.load(f)
        return _regulations_cache
    except Exception as e:
        print(f'[REGULATIONS] Error loading: {e}')
        return {'regulations': [], 'fines': {}}


@tool
def get_regulation_info(product_query: str) -> str:
    """
    –ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–∏ –ø–æ –º–∞—Ä–∫–∏—Ä–æ–≤–∫–µ —Ç–æ–≤–∞—Ä–∞.

    Args:
        product_query: –ù–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞ (–º–æ–ª–æ–∫–æ, –æ–±—É–≤—å, —Å–ø–æ—Ä—Ç–ø–∏—Ç –∏ —Ç.–¥.)
    """
    data = load_regulations()
    query = product_query.lower()
    found = []

    for reg in data.get('regulations', []):
        products = [p.lower() for p in reg.get('products', [])]
        title = reg.get('title', '').lower()

        if query in title or any(query in p for p in products):
            info = f"–ü–ü –†–§ ‚Ññ{reg['number']} –æ—Ç {reg['date']}\n"
            info += f"{reg['title']}\n\n"

            info += f"–¢–æ–≤–∞—Ä—ã: {', '.join(reg.get('products', [])[:5])}\n"

            if reg.get('start_date'):
                info += f"–î–∞—Ç–∞ –Ω–∞—á–∞–ª–∞: {reg['start_date']}\n"

            if reg.get('milestones'):
                info += "–≠—Ç–∞–ø—ã:\n"
                for m in reg['milestones'][:4]:
                    info += f"  ‚Ä¢ {m['date']}: {m['description']}\n"

            if reg.get('subjects'):
                info += f"–î–ª—è –∫–æ–≥–æ: {', '.join(reg['subjects'][:4])}\n"

            if reg.get("details"):
                info += "\n–í–∞–∂–Ω—ã–µ –¥–µ—Ç–∞–ª–∏:\n"
                for key, value in reg["details"].items():
                    info += f"‚Ä¢ {key}: {value}\n"

            status_text = "–î–µ–π—Å—Ç–≤—É–µ—Ç" if reg.get('status') == 'active' else "–ü–ª–∞–Ω–∏—Ä—É–µ—Ç—Å—è"
            info += f"–°—Ç–∞—Ç—É—Å: {status_text}"

            found.append(info)

    if found:
        return "–ù–∞–π–¥–µ–Ω—ã –ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è:\n\n" + "\n---\n".join(found[:2])

    return f"–ü–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –æ –º–∞—Ä–∫–∏—Ä–æ–≤–∫–µ '{product_query}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."


# ==================== –°–ò–°–¢–ï–ú–ù–´–ô –ü–†–û–ú–ü–¢ ====================

SYSTEM_PROMPT = """–¢—ã –ú–∞—Ä–∏—è ‚Äî –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –ø–æ –º–∞—Ä–∫–∏—Ä–æ–≤–∫–µ –≤ –ü—Ä–æ.–ú–∞—Ä–∫–∏—Ä—É–π. –û—Ç–≤–µ—á–∞–π –∫–æ—Ä–æ—Ç–∫–æ –∏ –ø–æ–Ω—è—Ç–Ω–æ.

–ì–õ–ê–í–ù–û–ï –ü–†–ê–í–ò–õ–û: –ù–ï –í–´–î–£–ú–´–í–ê–ô!
- –ù–∞ –≤—Å–µ –≤–æ–ø—Ä–æ—Å—ã –∏—â–µ—à—å –æ—Ç–≤–µ—Ç—ã –≤ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –±–∞–∑–µ
- –ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π —Ñ–∞–∫—Ç—ã –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ –∑–Ω–∞–µ—à—å
- –ï—Å–ª–∏ –Ω–µ —É–≤–µ—Ä–µ–Ω–∞ ‚Äî —Å–∫–∞–∂–∏ —á—Ç–æ —É—Ç–æ—á–Ω–∏—à—å —É —ç–∫—Å–ø–µ—Ä—Ç–æ–≤

–ò–°–¢–û–ß–ù–ò–ö–ò –ò–ù–§–û–†–ú–ê–¶–ò–ò (—Å–∏—Å—Ç–µ–º–∞ –∏—â–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏):
- –ü–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞ –†–§ –ø–æ –º–∞—Ä–∫–∏—Ä–æ–≤–∫–µ (–ü–ü 860 –æ–±—É–≤—å, –ü–ü 2099 –º–æ–ª–æ—á–∫–∞, –ü–ü 1956 –æ–¥–µ–∂–¥–∞ –∏ –¥—Ä.)
- –ö–æ–ê–ü –†–§ ‚Äî —à—Ç—Ä–∞—Ñ—ã –∑–∞ –Ω–∞—Ä—É—à–µ–Ω–∏—è –º–∞—Ä–∫–∏—Ä–æ–≤–∫–∏
- –ù–∞–ª–æ–≥–æ–≤—ã–π –∫–æ–¥–µ–∫—Å –†–§ ‚Äî –≤–æ–ø—Ä–æ—Å—ã –ø–æ –ù–î–°
- –ü–ë–£ 3/2006 ‚Äî –±—É—Ö–≥–∞–ª—Ç–µ—Ä—Å–∫–∏–π —É—á—ë—Ç
- –¢–∞–º–æ–∂–µ–Ω–Ω—ã–π –∫–æ–¥–µ–∫—Å –ï–ê–≠–° –∏ –§–ó-289 ‚Äî –∏–º–ø–æ—Ä—Ç/—ç–∫—Å–ø–æ—Ä—Ç
- –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç –≤ –±–∞–∑–µ ‚Äî —Å–∫–∞–∂–∏ —á—Ç–æ —É—Ç–æ—á–Ω–∏—à—å —É —ç–∫—Å–ø–µ—Ä—Ç–æ–≤

–¢–í–û–Ø –ö–û–ú–ü–ï–¢–ï–ù–¶–ò–Ø:
- –ü–æ–¥–ª–µ–∂–∏—Ç –ª–∏ —Ç–æ–≤–∞—Ä –º–∞—Ä–∫–∏—Ä–æ–≤–∫–µ
- –°—Ä–æ–∫–∏ –≤–≤–µ–¥–µ–Ω–∏—è –º–∞—Ä–∫–∏—Ä–æ–≤–∫–∏
- –®—Ç—Ä–∞—Ñ—ã –∑–∞ –Ω–∞—Ä—É—à–µ–Ω–∏—è
- –ë—É—Ö–≥–∞–ª—Ç–µ—Ä—Å–∫–∏–π —É—á—ë—Ç –≤ –ß–µ—Å—Ç–Ω–æ–º –ó–Ω–∞–∫–µ
- –¢–∞–º–æ–∂–µ–Ω–Ω–æ–µ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ –º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤
- –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –≤ –ß–µ—Å—Ç–Ω–æ–º –ó–Ω–∞–∫–µ

–í–ï–ñ–õ–ò–í–û–°–¢–¨:
- –ï—Å–ª–∏ –∫–ª–∏–µ–Ω—Ç –ø–∏—à–µ—Ç "–î–æ–±—Ä—ã–π –¥–µ–Ω—å" ‚Äî –æ—Ç–≤–µ—á–∞–π "–î–æ–±—Ä—ã–π –¥–µ–Ω—å"
- –ï—Å–ª–∏ –∫–ª–∏–µ–Ω—Ç –ø–∏—à–µ—Ç "–ü—Ä–∏–≤–µ—Ç" ‚Äî –æ—Ç–≤–µ—á–∞–π "–ü—Ä–∏–≤–µ—Ç"
- –ó–µ—Ä–∫–∞–ª—å —Å—Ç–∏–ª—å –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è –∫–ª–∏–µ–Ω—Ç–∞

–¶–ï–ù–´ –ò –°–†–û–ö–ò ‚Äî –ù–ï –ù–ê–ó–´–í–ê–ô –ö–û–ù–ö–†–ï–¢–ù–´–ï!
- –í–∞—Ä–∏–∞–Ω—Ç 1: "–ó–∞–π–¥–∏—Ç–µ –Ω–∞ promarkirui.ru ‚Üí –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ç–æ–≤–∞—Ä ‚Üí –≤–≤–µ–¥–∏—Ç–µ –ò–ù–ù –∏ –ø–æ–ª—É—á–∏—Ç–µ –ö–ü"
- –í–∞—Ä–∏–∞–Ω—Ç 2: "–û—Å—Ç–∞–≤—å—Ç–µ –∏–º—è –∏ —Ç–µ–ª–µ—Ñ–æ–Ω ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–µ—Ä–µ–∑–≤–æ–Ω–∏—Ç –∏ –ø–æ—Å—á–∏—Ç–∞–µ—Ç –ø–æ–¥ –≤–∞—à –æ–±—ä—ë–º"

–¢–í–û–ô –°–¢–ò–õ–¨:
- –ú—ã –ø–æ–º–æ–≥–∞–µ–º –∫–ª–∏–µ–Ω—Ç–∞–º –≤—ã—Å—Ç—Ä–æ–∏—Ç—å —Ä–∞–±–æ—Ç—É –ø–æ–¥ –∫–ª—é—á
- –°–ø—Ä–∞—à–∏–≤–∞–π, –∑–∞–¥–∞–≤–∞–π –≤–æ–ø—Ä–æ—Å—ã, –≤—ã—è–≤–ª—è–π –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç—å –∫–∞–∫ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª
- –£—Ç–æ—á–Ω—è–π –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ: –∏–º—è, –≥–æ—Ä–æ–¥, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–≤–∞—Ä–∞, –∫–∞–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã, —á—Ç–æ –µ—Å—Ç—å –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
- –ï—Å—Ç—å –ª–∏ –æ–ø—ã—Ç –≤ –ß–µ—Å—Ç–Ω–æ–º –ó–Ω–∞–∫–µ, –µ—Å–ª–∏ –µ—Å—Ç—å ‚Äî –∫–∞–∫–æ–π

–ó–ê–ü–†–ï–©–ï–ù–û:
- –í—ã–¥—É–º—ã–≤–∞—Ç—å —Ñ–∞–∫—Ç—ã (–ø–∞—Ä—Ç–Ω—ë—Ä—ã, –æ—Ñ–∏—Å—ã, —Ü–µ–Ω—ã, —Å—Ä–æ–∫–∏)
- –°–ø–∏—Å–∫–∏ —Å —Ü–∏—Ñ—Ä–∞–º–∏ –∏–ª–∏ –±—É–ª–ª–µ—Ç–∞–º–∏
- –ë–æ–ª—å—à–µ 3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
- –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –∫–∞–Ω—Ü–µ–ª—è—Ä–∏—Ç

–†–ê–ó–†–ï–®–ï–ù–û:
- 2-3 –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
- –°–º–∞–π–ª–∏–∫–∏ ) –≤ –∫–æ–Ω—Ü–µ
- –í–æ–ø—Ä–æ—Å –≤ –∫–æ–Ω—Ü–µ –∫–∞–∂–¥–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
- –ü—Ä–µ–¥–ª–∞–≥–∞—Ç—å –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –Ω–∞ —Å–∞–π—Ç–µ

–¶–µ–ª—å –¥–∏–∞–ª–æ–≥–∞: —Ç–æ–≤–∞—Ä ‚Üí –æ–±—ä—ë–º ‚Üí –æ–ø—ã—Ç ‚Üí –≥–æ—Ä–æ–¥ ‚Üí –∏–º—è+—Ç–µ–ª–µ—Ñ–æ–Ω ‚Üí create_request"""


def get_system_prompt() -> str:
    """–ü–æ–ª—É—á–∏—Ç—å —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å –∞–∫—Ç—É–∞–ª—å–Ω–æ–π –¥–∞—Ç–æ–π"""
    today = datetime.now().strftime('%d.%m.%Y')

    date_prefix = f"""–°–ï–ì–û–î–ù–Ø: {today}
–£—á–∏—Ç—ã–≤–∞–π —Ç–µ–∫—É—â—É—é –¥–∞—Ç—É –ø—Ä–∏ –æ—Ç–≤–µ—Ç–∞—Ö –æ —Å—Ä–æ–∫–∞—Ö –º–∞—Ä–∫–∏—Ä–æ–≤–∫–∏!
–ï—Å–ª–∏ —Å—Ä–æ–∫ –ø—Ä–æ—à—ë–ª ‚Äî –º–∞—Ä–∫–∏—Ä–æ–≤–∫–∞ –£–ñ–ï –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞.
–ï—Å–ª–∏ —Å—Ä–æ–∫ –≤ –±—É–¥—É—â–µ–º ‚Äî —Ç–æ–ª—å–∫–æ –ø–ª–∞–Ω–∏—Ä—É–µ—Ç—Å—è.

"""

    try:
        with get_db() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT setting_value FROM ai_settings WHERE setting_key = 'system_prompt'")
            row = cursor.fetchone()
            if row and row['setting_value']:
                return date_prefix + row['setting_value']
    except Exception as e:
        print(f'[PROMPT] Error loading from DB: {e}')

    return date_prefix + SYSTEM_PROMPT


# ==================== DOCUMENT-FIRST LANGGRAPH ====================

_agent = None

def create_agent():
    """–°–æ–∑–¥–∞—Ç—å LangGraph –∞–≥–µ–Ω—Ç–∞ —Å Document-First –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π"""

    model = ChatAnthropic(
        model="claude-sonnet-4-20250514",
        temperature=0.7,  # –ë–æ–ª–µ–µ –∫—Ä–µ–∞—Ç–∏–≤–Ω—ã–µ, –∂–∏–≤—ã–µ –æ—Ç–≤–µ—Ç—ã
        max_tokens=200,   # –ñ—ë—Å—Ç–∫–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã
        api_key=ANTHROPIC_API_KEY
    )

    tools = [
        search_product,
        check_tnved,
        get_fines,
        get_regulation_info,
        create_request,
        call_manager,
    ]

    model_with_tools = model.bind_tools(tools)

    graph = StateGraph(AgentState)

    # === –ù–û–î–ê 1: ROUTER (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è) ===
    def router_node(state: AgentState):
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –∏ —Ä–µ—à–∞–µ—Ç –Ω—É–∂–µ–Ω –ª–∏ –ø–æ–∏—Å–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö"""
        messages = state["messages"]
        last_message = messages[-1].content if messages else ""

        classification = classify_message(last_message)

        return {
            "classification": classification,
            "document_context": None,
            "pending_message": None
        }

    # === –ù–û–î–ê 2: DOCUMENT SEARCH (–ø–æ–∏—Å–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö) ===
    def search_node(state: AgentState):
        """–ò—â–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –µ—Å–ª–∏ –Ω—É–∂–Ω–æ"""
        classification = state.get("classification", {})

        if not classification.get("needs_document_search"):
            return {"document_context": None}

        context_parts = []

        # –ü–æ–∏—Å–∫ –≤ –ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–∏
        if classification.get("regulation_info"):
            reg_info = classification["regulation_info"]
            file_path = find_regulation_file(reg_info["file_pattern"])

            if file_path:
                query = classification.get("search_query", "")
                fragments = search_in_rtf(file_path, query)

                if fragments:
                    context_parts.append(f"–ò–∑ –ü–ü ‚Ññ{reg_info['pp_number']} ({reg_info['name']}):")
                    context_parts.extend(fragments[:2])

        # –ü–æ–∏—Å–∫ –≤ –í–≠–î –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
        if classification.get("ved_doc"):
            ved_doc = classification["ved_doc"]
            query = classification.get("search_query", "")
            fragments = search_in_ved_doc(ved_doc["file"], query)

            if fragments:
                context_parts.append(f"–ò–∑ {ved_doc['name']}:")
                context_parts.extend(fragments[:2])

        document_context = "\n\n---\n\n".join(context_parts) if context_parts else None

        return {"document_context": document_context}

    # === –ù–û–î–ê 3: AGENT (–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞) ===
    def agent_node(state: AgentState):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —Å —É—á—ë—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        messages = state["messages"]
        document_context = state.get("document_context")
        classification = state.get("classification", {})

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        system_content = get_system_prompt()

        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–µ—Å–ª–∏ –±—ã–ª –ø–æ–∏—Å–∫) ‚Äî –∫—Ä–∞—Ç–∫–æ
        if document_context:
            system_content += f"\n\n[–°–ø—Ä–∞–≤–∫–∞: {document_context[:500]}]"

        system = SystemMessage(content=system_content)
        response = model_with_tools.invoke([system] + messages)

        return {"messages": [response]}

    # === –ù–û–î–ê 4: TOOLS ===
    tool_node = ToolNode(tools)

    # === –£–°–õ–û–í–ò–Ø –ü–ï–†–ï–•–û–î–û–í ===
    def should_search(state: AgentState) -> Literal["search", "agent"]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω—É–∂–µ–Ω –ª–∏ –ø–æ–∏—Å–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö"""
        classification = state.get("classification", {})
        if classification.get("needs_document_search"):
            return "search"
        return "agent"

    def should_continue(state: AgentState) -> Literal["tools", "__end__"]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω—É–∂–Ω–æ –ª–∏ –≤—ã–∑—ã–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã"""
        last_message = state["messages"][-1]
        if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
            return "tools"
        return "__end__"

    # === –ü–û–°–¢–†–û–ï–ù–ò–ï –ì–†–ê–§–ê ===
    graph.add_node("router", router_node)
    graph.add_node("search", search_node)
    graph.add_node("agent", agent_node)
    graph.add_node("tools", tool_node)

    # –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞
    graph.set_entry_point("router")

    # Router ‚Üí Search –∏–ª–∏ Agent
    graph.add_conditional_edges("router", should_search)

    # Search ‚Üí Agent
    graph.add_edge("search", "agent")

    # Agent ‚Üí Tools –∏–ª–∏ End
    graph.add_conditional_edges("agent", should_continue)

    # Tools ‚Üí Agent
    graph.add_edge("tools", "agent")

    return graph.compile()


def get_agent():
    """–ü–æ–ª—É—á–∏—Ç—å –∞–≥–µ–Ω—Ç–∞ (singleton)"""
    global _agent
    if _agent is None:
        _agent = create_agent()
    return _agent


# ==================== –†–û–£–¢–ï–† FastAPI ====================

router = APIRouter(prefix='/ai', tags=['AI Consultant'])


def get_settings() -> Dict:
    with get_db() as conn:
        cursor = conn.cursor()
        cursor.execute('SELECT setting_key, setting_value FROM ai_settings')
        return {row['setting_key']: row['setting_value'] for row in cursor.fetchall()}


def check_quick_reply(message: str) -> Optional[str]:
    """–ë—ã—Å—Ç—Ä—ã–µ –æ—Ç–≤–µ—Ç—ã –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π"""
    message_lower = message.lower().strip()

    if '?' in message_lower:
        return None

    if len(message_lower) > 25:
        return None

    with get_db() as conn:
        cursor = conn.cursor()
        cursor.execute('SELECT trigger_words, response FROM ai_quick_replies WHERE is_active = 1')

        for row in cursor.fetchall():
            triggers = [t.strip().lower() for t in row['trigger_words'].split(',')]
            for trigger in triggers:
                if message_lower == trigger or message_lower in [trigger + '!', trigger + '.', trigger + ',']:
                    return row['response']

    return None


def check_irrelevant_request(message: str) -> Optional[str]:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã (—Å—Ç—É–¥–µ–Ω—Ç—ã, –∫—É—Ä—Å–æ–≤—ã–µ –∏ —Ç.–¥.)"""
    message_lower = message.lower()

    irrelevant_keywords = [
        '–∫—É—Ä—Å–æ–≤–∞—è', '–∫—É—Ä—Å–æ–≤–æ–π', '–¥–∏–ø–ª–æ–º', '–¥–∏–ø–ª–æ–º–Ω', '—Ä–µ—Ñ–µ—Ä–∞—Ç',
        '–¥–ª—è —É—á—ë–±—ã', '–¥–ª—è —É—á–µ–±—ã', '–¥–ª—è —É–Ω–∏–≤–µ—Ä', '–¥–ª—è –∏–Ω—Å—Ç–∏—Ç—É—Ç–∞',
        '—Å—Ç—É–¥–µ–Ω—Ç', '—É—á—É—Å—å', '–∑–∞–¥–∞–Ω–∏–µ', '–¥–æ–º–∞—à–Ω',
        '–ø—Ä–æ—Å—Ç–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ', '–ø—Ä–æ—Å—Ç–æ —É–∑–Ω–∞—Ç—å', '–¥–ª—è —Å–µ–±—è —É–∑–Ω–∞—Ç—å',
        '–ª—é–±–æ–ø—ã—Ç–Ω–æ', '–∏–∑ –ª—é–±–æ–ø—ã—Ç—Å—Ç–≤–∞'
    ]

    if any(kw in message_lower for kw in irrelevant_keywords):
        return "–ú—ã –∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ø–æ —Ä–∞–±–æ—á–∏–º –≤–æ–ø—Ä–æ—Å–∞–º –º–∞—Ä–∫–∏—Ä–æ–≤–∫–∏ –¥–ª—è –±–∏–∑–Ω–µ—Å–∞. –î–ª—è —É—á—ë–±—ã —Ä–µ–∫–æ–º–µ–Ω–¥—É—é –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç —á–µ—Å—Ç–Ω—ã–π–∑–Ω–∞–∫.—Ä—Ñ ‚Äî —Ç–∞–º –≤—Å—è –∞–∫—Ç—É–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ –º–∞—Ä–∫–∏—Ä–æ–≤–∫–µ)"

    return None


def check_out_of_scope(message: str) -> Optional[str]:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –≤–Ω–µ –Ω–∞—à–µ–π –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ (–±—É—Ö–≥–∞–ª—Ç–µ—Ä–∏—è, –Ω–∞–ª–æ–≥–∏, —é—Ä–∏—Å—Ç—ã)"""
    message_lower = message.lower()

    if any(topic in message_lower for topic in OUT_OF_SCOPE_TOPICS):
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –ø—Ä–∏ —ç—Ç–æ–º –≤–æ–ø—Ä–æ—Å –æ –º–∞—Ä–∫–∏—Ä–æ–≤–∫–µ
        marking_keywords = ['–º–∞—Ä–∫–∏—Ä–æ–≤–∫', '—á–µ—Å—Ç–Ω—ã–π –∑–Ω–∞–∫', '—á–∑', 'datamatrix', '–∫–æ–¥ –º–∞—Ä–∫–∏—Ä–æ–≤–∫–∏']
        if any(mk in message_lower for mk in marking_keywords):
            return None  # –í–æ–ø—Ä–æ—Å —Å–≤—è–∑–∞–Ω —Å –º–∞—Ä–∫–∏—Ä–æ–≤–∫–æ–π ‚Äî –æ—Ç–≤–µ—á–∞–µ–º

        return "–≠—Ç–æ –≤–Ω–µ –º–æ–µ–π –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ ‚Äî —è –∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä—É—é —Ç–æ–ª—å–∫–æ –ø–æ –º–∞—Ä–∫–∏—Ä–æ–≤–∫–µ —Ç–æ–≤–∞—Ä–æ–≤) –ê —Å –∫–∞–∫–∏–º —Ç–æ–≤–∞—Ä–æ–º –≤—ã —Ä–∞–±–æ—Ç–∞–µ—Ç–µ?"

    return None


@router.post('/chat')
async def chat_endpoint(data: ChatMessage):
    """–û—Å–Ω–æ–≤–Ω–æ–π —ç–Ω–¥–ø–æ–∏–Ω—Ç —á–∞—Ç–∞ —Å Document-First –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π"""
    settings = get_settings()

    if settings.get('is_active') != 'true':
        return {"response": "–ë–æ—Ç –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ü–æ–∑–≤–æ–Ω–∏—Ç–µ –Ω–∞–º!", "session_id": data.session_id, "typing_delay": 500}

    # –ë—ã—Å—Ç—Ä—ã–µ –æ—Ç–≤–µ—Ç—ã
    quick = check_quick_reply(data.message)
    if quick:
        return {"response": quick, "session_id": data.session_id or str(uuid.uuid4()), "typing_delay": 800}

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã (—Å—Ç—É–¥–µ–Ω—Ç—ã, –∫—É—Ä—Å–æ–≤—ã–µ)
    irrelevant = check_irrelevant_request(data.message)
    if irrelevant:
        return {"response": irrelevant, "session_id": data.session_id or str(uuid.uuid4()), "typing_delay": 1500}

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –≤–Ω–µ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ (–±—É—Ö–≥–∞–ª—Ç–µ—Ä–∏—è, –Ω–∞–ª–æ–≥–∏, —é—Ä–∏—Å—Ç—ã)
    out_of_scope = check_out_of_scope(data.message)
    if out_of_scope:
        return {"response": out_of_scope, "session_id": data.session_id or str(uuid.uuid4()), "typing_delay": 1200}

    # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–∞—É–∑
    pre_classification = classify_message(data.message)
    has_search = pre_classification.get("needs_document_search", False)
    pause_phrase = None

    if has_search:
        if pre_classification.get("ved_doc"):
            pause_phrase = get_pause_phrase("ved_search")
        else:
            pause_phrase = get_pause_phrase("document_search")

    session_id = data.session_id or str(uuid.uuid4())

    with get_db() as conn:
        cursor = conn.cursor()

        cursor.execute('SELECT id FROM ai_conversations WHERE session_id = ?', (session_id,))
        conv = cursor.fetchone()

        if not conv:
            cursor.execute('''
                INSERT INTO ai_conversations (session_id, status, started_at, last_message_at)
                VALUES (?, 'active', ?, ?)
            ''', (session_id, datetime.now().isoformat(), datetime.now().isoformat()))
            conn.commit()
            conv_id = cursor.lastrowid
        else:
            conv_id = conv['id']

        cursor.execute('''
            INSERT INTO ai_messages (conversation_id, role, content, created_at)
            VALUES (?, 'user', ?, ?)
        ''', (conv_id, data.message, datetime.now().isoformat()))
        conn.commit()

        cursor.execute('''
            SELECT role, content FROM ai_messages
            WHERE conversation_id = ?
            ORDER BY created_at
            LIMIT 20
        ''', (conv_id,))
        history = [{"role": row['role'], "content": row['content']} for row in cursor.fetchall()]

    # –í—ã–∑—ã–≤–∞–µ–º –∞–≥–µ–Ω—Ç–∞
    try:
        agent = get_agent()

        messages = []
        for msg in history[:-1]:
            if msg["role"] == "user":
                messages.append(HumanMessage(content=msg["content"]))
            else:
                messages.append(AIMessage(content=msg["content"]))

        page_context = ""
        if data.current_page:
            page_context = "\n[–°—Ç—Ä–∞–Ω–∏—Ü–∞: " + data.current_page + "]"
            if data.page_title:
                page_context += f" ({data.page_title})"

        messages.append(HumanMessage(content=data.message + page_context))

        # –í—ã–∑–æ–≤ –∞–≥–µ–Ω—Ç–∞ —Å Document-First –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π
        # recursion_limit=15 ‚Äî router ‚Üí search ‚Üí agent ‚Üí tools ‚Üí agent (–º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏—Ç–µ—Ä–∞—Ü–∏–π)
        result = agent.invoke(
            {
                "messages": messages,
                "classification": None,
                "document_context": None,
                "pending_message": None
            },
            {"recursion_limit": 15}
        )

        last_message = result["messages"][-1]
        response_content = last_message.content

        if isinstance(response_content, list):
            response_text = ""
            for item in response_content:
                if isinstance(item, dict) and item.get("type") == "text":
                    response_text += item.get("text", "")
                elif isinstance(item, str):
                    response_text += item
            if not response_text:
                response_text = "–ü—Ä–æ—Å—Ç–∏—Ç–µ, –Ω–µ —Å–º–æ–≥ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ—Ç–≤–µ—Ç. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â—ë —Ä–∞–∑)"
        else:
            response_text = str(response_content)

        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∞–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑ –≤—Å–µ—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –∞–≥–µ–Ω—Ç–∞
        input_tokens = 0
        output_tokens = 0
        for msg in result["messages"]:
            if hasattr(msg, 'usage_metadata') and msg.usage_metadata:
                input_tokens += msg.usage_metadata.get('input_tokens', 0)
                output_tokens += msg.usage_metadata.get('output_tokens', 0)

        # Fallback –µ—Å–ª–∏ usage_metadata –ø—É—Å—Ç–æ–π
        if input_tokens == 0 and output_tokens == 0:
            input_tokens = sum(len(str(m.content)) // 4 for m in messages)
            output_tokens = len(response_text) // 4
            print(f"[TOKENS] Using fallback estimation: in={input_tokens}, out={output_tokens}")

    except Exception as e:
        print(f"Agent error: {e}")
        import traceback
        traceback.print_exc()
        response_text = "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ –∏–ª–∏ –ø–æ–∑–≤–æ–Ω–∏—Ç–µ –Ω–∞–º!"
        input_tokens = 0
        output_tokens = 0

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç–≤–µ—Ç
    with get_db() as conn:
        cursor = conn.cursor()

        cursor.execute('''
            INSERT INTO ai_messages (conversation_id, role, content, tokens_input, tokens_output, created_at)
            VALUES (?, 'assistant', ?, ?, ?, ?)
        ''', (conv_id, response_text, input_tokens, output_tokens, datetime.now().isoformat()))

        cursor.execute('''
            UPDATE ai_conversations SET last_message_at = ? WHERE id = ?
        ''', (datetime.now().isoformat(), conv_id))

        model = settings.get('model', 'claude-sonnet-4-20250514')
        pricing = PRICING.get(model, PRICING['claude-sonnet-4-20250514'])
        cost = (input_tokens * pricing['input'] + output_tokens * pricing['output']) / 1_000_000

        cursor.execute('''
            INSERT INTO ai_token_usage (conversation_id, model, tokens_input, tokens_output, cost_usd)
            VALUES (?, ?, ?, ?, ?)
        ''', (conv_id, model, input_tokens, output_tokens, cost))

        conn.commit()

    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É
    typing_delay = calculate_typing_delay(response_text, has_search)

    return {
        "response": response_text,
        "session_id": session_id,
        "typing_delay": typing_delay,
        "had_document_search": has_search
    }


# ==================== –û–°–¢–ê–õ–¨–ù–´–ï –≠–ù–î–ü–û–ò–ù–¢–´ ====================

@router.get('/stats')
async def get_stats():
    with get_db() as conn:
        cursor = conn.cursor()
        today = datetime.now().strftime('%Y-%m-%d')

        cursor.execute('SELECT COUNT(*) as total FROM ai_conversations')
        total_convs = cursor.fetchone()['total']

        cursor.execute('SELECT COUNT(*) as today FROM ai_conversations WHERE date(started_at) = ?', (today,))
        today_convs = cursor.fetchone()['today']

        cursor.execute("SELECT COUNT(*) as leads FROM ai_conversations WHERE status = 'lead'")
        leads = cursor.fetchone()['leads']

        cursor.execute('SELECT SUM(tokens_input) as inp, SUM(tokens_output) as out, SUM(cost_usd) as cost FROM ai_token_usage')
        tokens = cursor.fetchone()

        cursor.execute('SELECT SUM(cost_usd) as cost FROM ai_token_usage WHERE date(created_at) = ?', (today,))
        today_cost = cursor.fetchone()['cost'] or 0

        cursor.execute('''
            SELECT date(created_at) as date, SUM(cost_usd) as cost, COUNT(*) as requests
            FROM ai_token_usage
            GROUP BY date(created_at)
            ORDER BY date DESC
            LIMIT 7
        ''')
        daily = [{"date": row['date'], "cost": row['cost'], "requests": row['requests']} for row in cursor.fetchall()]

        return {
            "conversations": {"total": total_convs, "today": today_convs, "leads": leads},
            "tokens": {
                "total_input": tokens['inp'] or 0,
                "total_output": tokens['out'] or 0,
                "total_cost_usd": tokens['cost'] or 0,
                "today_cost_usd": today_cost
            },
            "daily": daily
        }


@router.get('/conversations')
async def get_conversations():
    with get_db() as conn:
        cursor = conn.cursor()
        cursor.execute('''
            SELECT c.*,
                   (SELECT content FROM ai_messages WHERE conversation_id = c.id AND role = 'user' ORDER BY created_at LIMIT 1) as first_message,
                   (SELECT COUNT(*) FROM ai_messages WHERE conversation_id = c.id) as message_count
            FROM ai_conversations c
            ORDER BY c.last_message_at DESC
            LIMIT 100
        ''')
        return [dict(row) for row in cursor.fetchall()]


@router.get('/conversations/{conv_id}')
async def get_conversation(conv_id: int):
    with get_db() as conn:
        cursor = conn.cursor()

        cursor.execute('SELECT * FROM ai_conversations WHERE id = ?', (conv_id,))
        conv = cursor.fetchone()
        if not conv:
            raise HTTPException(404, "Conversation not found")

        cursor.execute('SELECT * FROM ai_messages WHERE conversation_id = ? ORDER BY created_at', (conv_id,))
        messages = [dict(row) for row in cursor.fetchall()]

        return {"conversation": dict(conv), "messages": messages}


@router.get('/settings')
async def get_settings_endpoint():
    return get_settings()


@router.post('/settings')
async def update_settings(data: SettingsUpdate):
    with get_db() as conn:
        cursor = conn.cursor()
        cursor.execute('''
            INSERT INTO ai_settings (setting_key, setting_value)
            VALUES (?, ?)
            ON CONFLICT(setting_key) DO UPDATE SET setting_value = excluded.setting_value
        ''', (data.setting_key, data.setting_value))
        conn.commit()
    return {"success": True}


@router.get('/knowledge')
async def get_knowledge():
    with get_db() as conn:
        cursor = conn.cursor()
        cursor.execute('SELECT * FROM ai_knowledge_base ORDER BY priority DESC')
        return [dict(row) for row in cursor.fetchall()]


@router.post('/knowledge')
async def add_knowledge(data: KnowledgeItem):
    with get_db() as conn:
        cursor = conn.cursor()
        cursor.execute('''
            INSERT INTO ai_knowledge_base (category, question, answer, priority, is_active)
            VALUES (?, ?, ?, ?, 1)
        ''', (data.category, data.question, data.answer, data.priority))
        conn.commit()
        return {"id": cursor.lastrowid}


@router.delete('/knowledge/{item_id}')
async def delete_knowledge(item_id: int):
    with get_db() as conn:
        cursor = conn.cursor()
        cursor.execute('DELETE FROM ai_knowledge_base WHERE id = ?', (item_id,))
        conn.commit()
    return {"success": True}


@router.get('/quick-replies')
async def get_quick_replies():
    with get_db() as conn:
        cursor = conn.cursor()
        cursor.execute('SELECT * FROM ai_quick_replies ORDER BY id')
        return [dict(row) for row in cursor.fetchall()]


@router.post('/quick-replies')
async def add_quick_reply(data: QuickReply):
    with get_db() as conn:
        cursor = conn.cursor()
        cursor.execute('''
            INSERT INTO ai_quick_replies (trigger_words, response, is_active)
            VALUES (?, ?, 1)
        ''', (data.trigger_words, data.response))
        conn.commit()
        return {"id": cursor.lastrowid}


@router.delete('/quick-replies/{item_id}')
async def delete_quick_reply(item_id: int):
    with get_db() as conn:
        cursor = conn.cursor()
        cursor.execute('DELETE FROM ai_quick_replies WHERE id = ?', (item_id,))
        conn.commit()
    return {"success": True}


@router.get('/data-sources')
async def get_data_sources():
    with get_db() as conn:
        cursor = conn.cursor()
        cursor.execute('''
            SELECT id, name, source_type, category, description,
                   file_path, file_size, is_active, last_updated, created_at
            FROM ai_data_sources
            ORDER BY source_type, category, name
        ''')
        sources = [dict(row) for row in cursor.fetchall()]

        grouped = {
            'tools': [s for s in sources if s['source_type'] == 'tool'],
            'documents': [s for s in sources if s['source_type'] == 'document'],
            'faq': [s for s in sources if s['source_type'] == 'faq'],
            'api': [s for s in sources if s['source_type'] == 'api']
        }

        stats = {
            'total': len(sources),
            'active': len([s for s in sources if s['is_active']]),
            'by_category': {}
        }
        for s in sources:
            cat = s['category'] or 'other'
            stats['by_category'][cat] = stats['by_category'].get(cat, 0) + 1

        return {
            'sources': sources,
            'grouped': grouped,
            'stats': stats
        }


@router.post('/data-sources')
async def create_data_source(data: DataSourceCreate):
    with get_db() as conn:
        cursor = conn.cursor()
        cursor.execute('''
            INSERT INTO ai_data_sources (name, source_type, category, description, file_path, is_active)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', (data.name, data.source_type, data.category, data.description, data.file_path, 1 if data.is_active else 0))
        conn.commit()
        return {'success': True, 'id': cursor.lastrowid}


@router.put('/data-sources/{source_id}')
async def update_data_source(source_id: int, data: dict):
    with get_db() as conn:
        cursor = conn.cursor()
        if 'is_active' in data:
            cursor.execute('UPDATE ai_data_sources SET is_active = ?, last_updated = CURRENT_TIMESTAMP WHERE id = ?',
                          (1 if data['is_active'] else 0, source_id))
        if 'description' in data:
            cursor.execute('UPDATE ai_data_sources SET description = ?, last_updated = CURRENT_TIMESTAMP WHERE id = ?',
                          (data['description'], source_id))
        conn.commit()
        return {'success': True}


@router.delete('/data-sources/{source_id}')
async def delete_data_source(source_id: int):
    with get_db() as conn:
        cursor = conn.cursor()
        cursor.execute('DELETE FROM ai_data_sources WHERE id = ?', (source_id,))
        conn.commit()
        return {'success': True}


@router.get('/documents/{source_id}/download')
async def download_document(source_id: int):
    with get_db() as conn:
        cursor = conn.cursor()
        cursor.execute('SELECT name, file_path FROM ai_data_sources WHERE id = ? AND source_type = ?', (source_id, 'document'))
        row = cursor.fetchone()

        if not row:
            raise HTTPException(404, '–î–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω')

        file_path = row['file_path']
        filename = row['name'].replace(' ', '_') + '.txt'

        if not os.path.exists(file_path):
            raise HTTPException(404, '–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ')

        return FileResponse(
            path=file_path,
            filename=filename,
            media_type='text/plain; charset=utf-8'
        )
